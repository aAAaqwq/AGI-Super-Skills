#!/usr/bin/env python3
"""
å…è´¹å¤šå¹³å°è½®æ¢çˆ¬è™«
åœ¨å¤©çœ¼æŸ¥ã€ä¼æŸ¥æŸ¥ã€çˆ±ä¼æŸ¥ã€å›½å®¶å…¬ç¤ºç³»ç»Ÿä¹‹é—´è½®æ¢ï¼Œæœ€å¤§åŒ–å…è´¹é¢åº¦
"""

import json
import os
import re
import asyncio
import random
from datetime import datetime
from pathlib import Path
from urllib.parse import quote

# å°è¯•å¯¼å…¥ playwright
try:
    from playwright.async_api import async_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False
    print("âš ï¸ Playwright æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ‰‹åŠ¨æ¨¡å¼")


class FreePlatformRotator:
    """å…è´¹å¹³å°è½®æ¢å™¨"""
    
    # å¹³å°é…ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    PLATFORMS = [
        {
            "name": "å¤©çœ¼æŸ¥",
            "id": "tianyancha",
            "search_url": "https://www.tianyancha.com/search?key={query}",
            "daily_limit": 20,
            "delay_range": (3, 8),
            "selectors": {
                "result_link": "a[href*='/company/']",
                "company_name": ".company-name",
                "legal_person": ".legal-person",
                "capital": ".capital",
                "established": ".establish-date"
            }
        },
        {
            "name": "ä¼æŸ¥æŸ¥",
            "id": "qichacha",
            "search_url": "https://www.qcc.com/search?key={query}",
            "daily_limit": 10,
            "delay_range": (5, 12),
            "selectors": {
                "result_link": "a[href*='/firm/']",
                "company_name": ".company-name",
                "legal_person": ".legal-person",
                "capital": ".capital",
                "established": ".establish-date"
            }
        },
        {
            "name": "çˆ±ä¼æŸ¥",
            "id": "aiqicha",
            "search_url": "https://aiqicha.baidu.com/s?q={query}",
            "daily_limit": 50,  # ç™¾åº¦ç³»çš„ï¼Œé™åˆ¶è¾ƒæ¾
            "delay_range": (2, 6),
            "selectors": {
                "result_link": "a[href*='/company/']",
            }
        },
        {
            "name": "å›½å®¶ä¼ä¸šä¿¡ç”¨ä¿¡æ¯å…¬ç¤ºç³»ç»Ÿ",
            "id": "gsxt",
            "search_url": "https://www.gsxt.gov.cn/",
            "daily_limit": 999,  # å®˜æ–¹ç³»ç»Ÿï¼Œæ— é™åˆ¶
            "delay_range": (3, 10),
            "note": "éœ€è¦ç²¾ç¡®å…¬å¸åç§°æœç´¢"
        }
    ]
    
    def __init__(self):
        self.data_dir = Path(__file__).parent.parent / "data" / "snapshots"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨ç»Ÿè®¡
        self.stats_file = self.data_dir.parent / "usage_stats.json"
        self.stats = self._load_stats()
    
    def _load_stats(self):
        """åŠ è½½ä½¿ç”¨ç»Ÿè®¡"""
        if self.stats_file.exists():
            with open(self.stats_file, "r") as f:
                return json.load(f)
        return {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "platforms": {}
        }
    
    def _save_stats(self):
        """ä¿å­˜ä½¿ç”¨ç»Ÿè®¡"""
        with open(self.stats_file, "w") as f:
            json.dump(self.stats, f, indent=2)
    
    def _reset_daily_stats(self):
        """é‡ç½®æ¯æ—¥ç»Ÿè®¡"""
        today = datetime.now().strftime("%Y-%m-%d")
        if self.stats.get("date") != today:
            self.stats = {
                "date": today,
                "platforms": {}
            }
            self._save_stats()
    
    def _get_available_platforms(self):
        """è·å–å½“å‰å¯ç”¨çš„å¹³å°"""
        self._reset_daily_stats()
        available = []
        
        for platform in self.PLATFORMS:
            usage = self.stats["platforms"].get(platform["id"], 0)
            if usage < platform["daily_limit"]:
                available.append(platform)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæœªä½¿ç”¨çš„ä¼˜å…ˆï¼‰
        available.sort(key=lambda p: self.stats["platforms"].get(p["id"], 0))
        return available
    
    def _record_usage(self, platform_id):
        """è®°å½•ä½¿ç”¨æ¬¡æ•°"""
        if platform_id not in self.stats["platforms"]:
            self.stats["platforms"][platform_id] = 0
        self.stats["platforms"][platform_id] += 1
        self._save_stats()
    
    def get_usage_report(self):
        """è·å–ä½¿ç”¨æŠ¥å‘Š"""
        self._reset_daily_stats()
        report = []
        report.append(f"ğŸ“Š **ä»Šæ—¥ä½¿ç”¨ç»Ÿè®¡** ({self.stats['date']})\n")
        
        for platform in self.PLATFORMS:
            pid = platform["id"]
            used = self.stats["platforms"].get(pid, 0)
            limit = platform["daily_limit"]
            remaining = max(0, limit - used)
            pct = int(used / limit * 100) if limit < 999 else 0
            
            status = "ğŸŸ¢" if remaining > 5 else "ğŸŸ¡" if remaining > 0 else "ğŸ”´"
            report.append(f"{status} **{platform['name']}**: {used}/{limit} å·²ç”¨ ({remaining} æ¬¡å‰©ä½™)")
        
        return "\n".join(report)
    
    async def scrape_company(self, company_name):
        """
        é‡‡é›†ä¼ä¸šä¿¡æ¯ï¼Œè‡ªåŠ¨è½®æ¢å¹³å°
        è¿”å›: {
            "success": True/False,
            "source": "å¹³å°å",
            "data": {...},
            "platform_used": "å¹³å°ID"
        }
        """
        if not HAS_PLAYWRIGHT:
            return self._manual_fallback(company_name)
        
        available = self._get_available_platforms()
        
        if not available:
            return {
                "success": False,
                "error": "æ‰€æœ‰å¹³å°ä»Šæ—¥å…è´¹é¢åº¦å·²ç”¨å®Œ",
                "usage_report": self.get_usage_report()
            }
        
        # ä¾æ¬¡å°è¯•å¯ç”¨å¹³å°
        for platform in available:
            try:
                result = await self._scrape_from_platform(platform, company_name)
                if result["success"]:
                    self._record_usage(platform["id"])
                    return result
            except Exception as e:
                print(f"âš ï¸ {platform['name']} é‡‡é›†å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰å¹³å°éƒ½å¤±è´¥ï¼Œè¿”å›æ‰‹åŠ¨æŒ‡ä»¤
        return self._manual_fallback(company_name)
    
    async def _scrape_from_platform(self, platform, company_name):
        """ä»æŒ‡å®šå¹³å°é‡‡é›†"""
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            # éšæœºå»¶è¿Ÿ
            delay_min, delay_max = platform["delay_range"]
            await asyncio.sleep(random.uniform(delay_min, delay_max))
            
            # æœç´¢
            search_url = platform["search_url"].format(query=quote(company_name))
            
            try:
                await page.goto(search_url, wait_until="domcontentloaded", timeout=20000)
                await asyncio.sleep(random.uniform(2, 5))
                
                # å›½å®¶å…¬ç¤ºç³»ç»Ÿç‰¹æ®Šå¤„ç†
                if platform["id"] == "gsxt":
                    data = await self._scrape_gsxt(page, company_name)
                else:
                    data = await self._scrape_generic(page, platform, company_name)
                
                await browser.close()
                
                if data:
                    return {
                        "success": True,
                        "source": platform["name"],
                        "platform_used": platform["id"],
                        "data": data
                    }
                else:
                    return {"success": False, "error": "æœªæ‰¾åˆ°ä¼ä¸šä¿¡æ¯"}
                    
            except Exception as e:
                await browser.close()
                raise e
    
    async def _scrape_generic(self, page, platform, company_name):
        """é€šç”¨é‡‡é›†é€»è¾‘"""
        # ç‚¹å‡»ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
        link = await page.query_selector(platform["selectors"]["result_link"])
        if not link:
            return None
        
        await link.click()
        await asyncio.sleep(random.uniform(3, 8))
        
        # è§£æé¡µé¢
        content = await page.content()
        return self._parse_html_content(content, company_name)
    
    async def _scrape_gsxt(self, page, company_name):
        """å›½å®¶å…¬ç¤ºç³»ç»Ÿç‰¹æ®Šé‡‡é›†"""
        # è¾“å…¥å…¬å¸åç§°
        try:
            await page.fill('input[name="keyword"]', company_name)
            await page.click('button:has-text("æœç´¢")')
            await asyncio.sleep(5)
            
            # ç‚¹å‡»æœç´¢ç»“æœ
            result = await page.query_selector('a[href*="/detail"]')
            if result:
                await result.click()
                await asyncio.sleep(5)
                
                content = await page.content()
                return self._parse_html_content(content, company_name)
        except:
            pass
        
        return None
    
    def _parse_html_content(self, html, company_name):
        """ä» HTML è§£æä¼ä¸šæ•°æ®"""
        data = {"name": company_name}
        
        # ä½¿ç”¨æ­£åˆ™æå–å…³é”®ä¿¡æ¯
        capital = re.search(r'æ³¨å†Œèµ„æœ¬[ï¼š:]?\s*([\d,.]+)\s*(ä¸‡|äº¿)?[å…ƒäººæ°‘å¸]*', html)
        if capital:
            amount = capital.group(1).replace(",", "")
            unit = capital.group(2) or "ä¸‡"
            data["capital"] = f"{amount}{unit}"
            data["capital_amount"] = float(amount) * (10000 if unit == "ä¸‡" else 100000000)
        
        legal = re.search(r'æ³•å®šä»£è¡¨äºº[ï¼š:]\s*([^\s\n<>]{2,20})', html)
        if legal:
            data["legal_representative"] = legal.group(1).strip()
        
        date = re.search(r'æˆç«‹æ—¥æœŸ[ï¼š:]?\s*(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]?\d{0,2}æ—¥?)', html)
        if date:
            data["established_date"] = date.group(1)
        
        credit = re.search(r'ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç [ï¼š:]?\s*([A-Z0-9]{18})', html)
        if credit:
            data["credit_code"] = credit.group(1)
        
        # è§£æè‚¡ä¸œä¿¡æ¯
        shareholders = re.findall(r'è‚¡ä¸œåç§°[ï¼š:]?\s*([^\n<>]+?)(?=\s*æŒè‚¡)', html)
        if shareholders:
            data["shareholders"] = [
                {"name": s.strip(), "ratio": "æœªçŸ¥", "type": "æœªçŸ¥"}
                for s in shareholders[:5]
            ]
        
        data["fetched_at"] = datetime.now().isoformat()
        data["source"] = "auto_scrape"
        
        return data
    
    def _manual_fallback(self, company_name):
        """æ‰‹åŠ¨é‡‡é›†å›é€€æ–¹æ¡ˆ"""
        return {
            "success": False,
            "error": "éœ€è¦æ‰‹åŠ¨é‡‡é›†",
            "manual_instructions": {
                "company": company_name,
                "platforms": [
                    {
                        "name": "å¤©çœ¼æŸ¥",
                        "url": f"https://www.tianyancha.com/search?key={quote(company_name)}",
                        "priority": 1
                    },
                    {
                        "name": "ä¼æŸ¥æŸ¥",
                        "url": f"https://www.qcc.com/search?key={quote(company_name)}",
                        "priority": 2
                    },
                    {
                        "name": "å›½å®¶å…¬ç¤ºç³»ç»Ÿ",
                        "url": "https://www.gsxt.gov.cn/",
                        "priority": 3,
                        "note": "éœ€è¦ç²¾ç¡®æœç´¢"
                    }
                ],
                "fields_to_collect": [
                    "å…¬å¸å…¨ç§°",
                    "æ³•å®šä»£è¡¨äºº",
                    "æ³¨å†Œèµ„æœ¬",
                    "æˆç«‹æ—¥æœŸ",
                    "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ",
                    "è‚¡ä¸œä¿¡æ¯ï¼ˆåç§°ã€æŒè‚¡æ¯”ä¾‹ï¼‰"
                ],
                "save_to": str(self.data_dir / f"{company_name}.json")
            },
            "usage_report": self.get_usage_report()
        }
    
    def save_snapshot(self, company_name, data):
        """ä¿å­˜å¿«ç…§"""
        data["fetched_at"] = datetime.now().isoformat()
        
        filepath = self.data_dir / f"{company_name}.json"
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return filepath


# CLI æ¥å£
async def main():
    import sys
    
    rotator = FreePlatformRotator()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "status":
            print(rotator.get_usage_report())
        
        elif command == "scrape" and len(sys.argv) > 2:
            company_name = sys.argv[2]
            print(f"ğŸ” é‡‡é›†ä¼ä¸š: {company_name}")
            
            result = await rotator.scrape_company(company_name)
            
            if result["success"]:
                print(f"âœ… é‡‡é›†æˆåŠŸ (æ¥æº: {result['source']})")
                rotator.save_snapshot(company_name, result["data"])
                print(json.dumps(result["data"], indent=2, ensure_ascii=False))
            else:
                print(f"âŒ {result['error']}")
                if "manual_instructions" in result:
                    print("\nğŸ“‹ æ‰‹åŠ¨é‡‡é›†æŒ‡å—:")
                    for platform in result["manual_instructions"]["platforms"]:
                        print(f"  {platform['priority']}. {platform['name']}: {platform['url']}")
        
        else:
            print("""
ç”¨æ³•:
  python3 scraper_free.py status          # æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡
  python3 scraper_free.py scrape <å…¬å¸å>  # é‡‡é›†ä¼ä¸šä¿¡æ¯
            """)
    else:
        print(rotator.get_usage_report())


if __name__ == "__main__":
    asyncio.run(main())
