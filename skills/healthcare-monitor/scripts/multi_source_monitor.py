#!/usr/bin/env python3
"""
åŒ»ç–—èèµ„ç›‘æ§ç³»ç»Ÿ - å¤šæºæ•°æ®èåˆç›‘æ§
æ•´åˆå®˜æ–¹ã€å•†ä¸šã€åª’ä½“ã€æ‹›è˜ç­‰å¤šæ•°æ®æº
"""

import json
import subprocess
import urllib.request
from datetime import datetime, timedelta
from html.parser import HTMLParser

# ==================== é…ç½® ====================

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
TIMEOUT = 15

# ==================== å®˜æ–¹æ•°æ®æº ====================

class OfficialSourceScraper:
    """å›½å®¶ä¼ä¸šä¿¡ç”¨ä¿¡æ¯å…¬ç¤ºç³»ç»Ÿçˆ¬è™«"""
    
    def search_company(self, company_name):
        """æœç´¢ä¼ä¸šä¿¡æ¯"""
        # ç”±äºå®˜æ–¹ç³»ç»Ÿéœ€è¦éªŒè¯ç ï¼Œè¿™é‡Œæä¾›æ¥å£è¯´æ˜
        # å®é™…å®ç°éœ€è¦ Playwright + éªŒè¯ç è¯†åˆ«
        return {
            "source": "å›½å®¶ä¼ä¸šä¿¡ç”¨ä¿¡æ¯å…¬ç¤ºç³»ç»Ÿ",
            "available": True,
            "cost": 0,
            "data": None,
            "notes": "éœ€è¦éªŒè¯ç å¤„ç†ï¼Œå»ºè®®ä½¿ç”¨ Playwright"
        }

# ==================== å•†ä¸šæ•°æ®æº ====================

class TianyanchaScraper:
    """å¤©çœ¼æŸ¥æ•°æ®æº"""
    
    def search_company(self, company_name):
        """æœç´¢ä¼ä¸šä¿¡æ¯"""
        try:
            url = f"https://www.tianyancha.com/search?key={urllib.parse.quote(company_name)}"
            req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
            with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
                html = resp.read().decode("utf-8", errors="ignore")
            
            # è§£æä¼ä¸šåŸºæœ¬ä¿¡æ¯
            data = self._parse_company_info(html)
            return {
                "source": "å¤©çœ¼æŸ¥",
                "available": True,
                "cost": 0,
                "data": data,
                "url": url
            }
        except Exception as e:
            return {
                "source": "å¤©çœ¼æŸ¥",
                "available": False,
                "error": str(e)
            }
    
    def _parse_company_info(self, html):
        """è§£æä¼ä¸šä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„è§£æ
        return {
            "company_name": "",
            "registered_capital": "",
            "founding_date": "",
            "shareholders": [],
            "changes": []
        }

class QichachaScraper:
    """ä¼æŸ¥æŸ¥æ•°æ®æº"""
    
    def search_company(self, company_name):
        """æœç´¢ä¼ä¸šä¿¡æ¯"""
        try:
            url = f"https://www.qcc.com/web/search?key={urllib.parse.quote(company_name)}"
            req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
            with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
                html = resp.read().decode("utf-8", errors="ignore")
            
            return {
                "source": "ä¼æŸ¥æŸ¥",
                "available": True,
                "cost": 0,
                "data": {},
                "url": url
            }
        except Exception as e:
            return {
                "source": "ä¼æŸ¥æŸ¥",
                "available": False,
                "error": str(e)
            }

# ==================== åª’ä½“æ•°æ®æº ====================

class MediaScraper:
    """åª’ä½“èèµ„æ•°æ®æº"""
    
    def search_funding_news(self, company_name):
        """æœç´¢èèµ„ç›¸å…³æ–°é—»"""
        sources = [
            ("36æ°ª", f"https://36kr.com/search/articles/{company_name}"),
            ("åŠ¨è„‰ç½‘", f"https://www.vbdata.cn/search?keyword={company_name}"),
            ("æŠ•ä¸­ç½‘", f"https://www.chinaventure.com.cn/search/{company_name}")
        ]
        
        results = []
        for name, url in sources:
            try:
                req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
                with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
                    html = resp.read().decode("utf-8", errors="ignore")
                
                # è§£ææ–°é—»ï¼ˆç®€åŒ–ç‰ˆï¼‰
                news_count = html.count("èèµ„") + html.count("æŠ•èµ„")
                if news_count > 0:
                    results.append({
                        "source": name,
                        "url": url,
                        "news_count": news_count
                    })
            except Exception as e:
                pass
        
        return results

# ==================== æ‹›è˜æ•°æ®æº ====================

class HiringScraper:
    """æ‹›è˜æ•°æ®æº"""
    
    def search_hiring(self, company_name):
        """æœç´¢æ‹›è˜ä¿¡æ¯"""
        sources = [
            ("BOSSç›´è˜", f"https://www.zhipin.com/job_detail/?query={company_name}"),
            ("æ‹‰å‹¾ç½‘", f"https://www.lagou.com/jobs/list_{company_name}")
        ]
        
        results = []
        for name, url in sources:
            try:
                req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
                with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
                    html = resp.read().decode("utf-8", errors="ignore")
                
                # ç»Ÿè®¡å²—ä½æ•°é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
                job_count = html.count("èŒä½") + html.count("å²—ä½")
                results.append({
                    "source": name,
                    "url": url,
                    "job_count": job_count
                })
            except Exception as e:
                pass
        
        return results

# ==================== å¤šæºèåˆåˆ†æ ====================

class MultiSourceAnalyzer:
    """å¤šæºæ•°æ®èåˆåˆ†æå™¨"""
    
    def __init__(self):
        self.official_scraper = OfficialSourceScraper()
        self.tianyancha = TianyanchaScraper()
        self.qichacha = QichachaScraper()
        self.media = MediaScraper()
        self.hiring = HiringScraper()
    
    def analyze_company(self, company_name):
        """ç»¼åˆåˆ†æä¼ä¸šèèµ„ä¿¡å·"""
        
        print(f"\nğŸ” åˆ†æä¼ä¸š: {company_name}")
        print("=" * 50)
        
        # 1. å®˜æ–¹æ•°æ®æºï¼ˆä¸€çº§éªŒè¯ï¼‰
        print("\n[ä¸€çº§éªŒè¯] å®˜æ–¹æ•°æ®æº")
        official_data = self.official_scraper.search_company(company_name)
        print(f"  å›½å®¶ä¼ä¸šä¿¡ç”¨å…¬ç¤º: {'âœ…' if official_data['available'] else 'âŒ'}")
        
        # 2. å•†ä¸šæ•°æ®æºï¼ˆä¸€çº§éªŒè¯ - å¤šæºå¯¹æ¯”ï¼‰
        print("\n[ä¸€çº§éªŒè¯] å•†ä¸šæ•°æ®æº")
        tianyancha_data = self.tianyancha.search_company(company_name)
        qichacha_data = self.qichacha.search_company(company_name)
        
        tianyancha_available = tianyancha_data.get("available", False)
        qichacha_available = qichacha_data.get("available", False)
        
        print(f"  å¤©çœ¼æŸ¥: {'âœ…' if tianyancha_available else 'âŒ'}")
        print(f"  ä¼æŸ¥æŸ¥: {'âœ…' if qichacha_available else 'âŒ'}")
        
        # 3. åª’ä½“éªŒè¯ï¼ˆäºŒçº§éªŒè¯ï¼‰
        print("\n[äºŒçº§éªŒè¯] åª’ä½“éªŒè¯")
        media_results = self.media.search_funding_news(company_name)
        for media in media_results:
            print(f"  {media['source']}: {media['news_count']} æ¡ç›¸å…³")
        
        # 4. æ‹›è˜éªŒè¯ï¼ˆäºŒçº§éªŒè¯ï¼‰
        print("\n[äºŒçº§éªŒè¯] æ‹›è˜æ•°æ®")
        hiring_results = self.hiring.search_hiring(company_name)
        for hiring in hiring_results:
            print(f"  {hiring['source']}: {hiring['job_count']} ä¸ªå²—ä½")
        
        # 5. ç»¼åˆè¯„åˆ†
        print("\nğŸ“Š ç»¼åˆè¯„åˆ†")
        
        score = 0
        total = 7
        
        # ä¸€çº§éªŒè¯ï¼ˆ3 åˆ†ï¼‰
        if official_data.get("available"):
            score += 1
            print("  âœ“ å®˜æ–¹æ•°æ®æº: +1")
        
        if tianyancha_available and qichacha_available:
            score += 1
            print("  âœ“ å¤šæºå¯¹æ¯”: +1")
        
        # äºŒçº§éªŒè¯ï¼ˆ4 åˆ†ï¼‰
        if len(media_results) >= 2:
            score += 2
            print(f"  âœ“ åª’ä½“éªŒè¯: +2 ({len(media_results)} ä¸ªæ¥æº)")
        elif len(media_results) >= 1:
            score += 1
            print(f"  âœ“ åª’ä½“éªŒè¯: +1 ({len(media_results)} ä¸ªæ¥æº)")
        
        if hiring_results and hiring_results[0].get("job_count", 0) > 5:
            score += 2
            print(f"  âœ“ æ‹›è˜éªŒè¯: +2 (æ‰©æ‹›ä¿¡å·)")
        elif hiring_results and hiring_results[0].get("job_count", 0) > 0:
            score += 1
            print(f"  âœ“ æ‹›è˜éªŒè¯: +1 (æœ‰æ‹›è˜)")
        
        confidence = (score / total) * 100
        
        print(f"\n  ç»¼åˆå¾—åˆ†: {score}/{total}")
        print(f"  ç½®ä¿¡åº¦: {confidence:.0f}%")
        
        # ç»“è®º
        if confidence >= 80:
            conclusion = "âœ… çœŸå®åº¦é«˜ï¼Œå»ºè®®è·Ÿè¿›"
        elif confidence >= 60:
            conclusion = "ğŸŸ¡ è¾ƒçœŸå®ï¼Œéœ€è¦å…³æ³¨"
        elif confidence >= 40:
            conclusion = "ğŸŸ  å¯ç–‘ï¼Œéœ€è¦æ·±å…¥éªŒè¯"
        else:
            conclusion = "âŒ ä¸ç¡®å®šï¼Œä¸å»ºè®®è·Ÿè¿›"
        
        print(f"\n  ç»“è®º: {conclusion}")
        
        return {
            "company": company_name,
            "score": score,
            "total": total,
            "confidence": confidence,
            "conclusion": conclusion,
            "sources": {
                "official": official_data.get("available", False),
                "tianyancha": tianyancha_available,
                "qichacha": qichacha_available,
                "media_count": len(media_results),
                "hiring_count": len(hiring_results)
            }
        }

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    analyzer = MultiSourceAnalyzer()
    
    # æµ‹è¯•ä¼ä¸š
    companies = [
        "è¿ˆç‘åŒ»ç–—",
        "è”å½±åŒ»ç–—",
        "ç™¾æµç¥å·"
    ]
    
    results = []
    for company in companies:
        result = analyzer.analyze_company(company)
        results.append(result)
        print("\n" + "="*50 + "\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "companies": results
    }
    
    report_file = f"/home/aa/clawd/skills/healthcare-monitor/data/reports/multi_source_{datetime.now().strftime('%Y%m%d')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return results

if __name__ == "__main__":
    main()
