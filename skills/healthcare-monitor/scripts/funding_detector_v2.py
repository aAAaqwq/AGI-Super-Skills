#!/usr/bin/env python3
"""
åŒ»ç–—ä¼ä¸šèèµ„æ£€æµ‹å™¨ v2.0
å¤šæ•°æ®æºç‰ˆæœ¬ï¼š
1. Firecrawl API - æœç´¢å¼•æ“èšåˆ
2. 36æ°ª - åˆ›æŠ•æ–°é—»
3. åŠ¨è„‰ç½‘ - åŒ»ç–—å‚ç›´åª’ä½“
4. ITæ¡”å­ - æŠ•èèµ„æ•°æ®åº“
5. çˆ±ä¼æŸ¥ - å·¥å•†ä¿¡æ¯ (ç™¾åº¦)
6. æ–°æµªè´¢ç» - è´¢ç»æ–°é—»
"""

import json
import os
import re
import subprocess
import sys
import time
import random
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import quote
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("âš ï¸ è¯·å®‰è£… requests å’Œ beautifulsoup4: pip install requests beautifulsoup4", file=sys.stderr)

# é…ç½®
SKILL_DIR = Path(__file__).parent.parent
CONFIG_DIR = SKILL_DIR / "config"
DATA_DIR = SKILL_DIR / "data"

# User-Agent æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
]

# èèµ„å…³é”®è¯
FUNDING_KEYWORDS = [
    "èèµ„", "æŠ•èµ„", "è·æŠ•", "å®Œæˆèèµ„", "å®£å¸ƒèèµ„",
    "Aè½®", "Bè½®", "Cè½®", "Dè½®", "Eè½®", "Fè½®",
    "Pre-A", "Pre-B", "å¤©ä½¿è½®", "ç§å­è½®",
    "æˆ˜ç•¥æŠ•èµ„", "è‚¡æƒèèµ„", "å¢èµ„", "å…¥è‚¡",
    "é¢†æŠ•", "è·ŸæŠ•", "ä¼°å€¼", "IPO"
]

# æŠ•èµ„æœºæ„å…³é”®è¯
INVESTOR_KEYWORDS = [
    "èµ„æœ¬", "æŠ•èµ„", "åŸºé‡‘", "åˆ›æŠ•", "é£æŠ•", "VC", "PE",
    "çº¢æ‰", "é«˜ç“´", "IDG", "ç»çº¬", "å¯æ˜", "è½¯é“¶", "è…¾è®¯æŠ•èµ„",
    "é˜¿é‡Œå¥åº·", "ç™¾åº¦é£æŠ•", "å­—èŠ‚è·³åŠ¨", "ç¾å›¢é¾™ç ",
    "å›è”", "è¾¾æ™¨", "æ·±åˆ›æŠ•", "åŒåˆ›ä¼Ÿä¸š", "åŒ—æå…‰",
    "GGV", "æºç ", "äº”æº", "äº‘é”‹", "æ·¡é©¬é”¡"
]

# å™ªéŸ³å…³é”®è¯ (éœ€æ’é™¤)
NOISE_KEYWORDS = [
    "èèµ„èåˆ¸", "èèµ„ä½™é¢", "èèµ„ä¹°å…¥", "èèµ„å‡€ä¹°å…¥",
    "èèµ„å‡€å¿è¿˜", "ä¸¤è", "èèµ„å®¢", "èèµ„ç›˜",
    "æ¶¨åœ", "è·Œåœ", "è‚¡ä»·", "å¸‚å€¼è’¸å‘", "è‚¡ç¥¨"
]

# åŒ»ç–—è¡Œä¸šå…³é”®è¯ (ç”¨äºéªŒè¯ç›¸å…³æ€§)
HEALTHCARE_KEYWORDS = [
    "åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "åŸºå› ", "è¯Šæ–­", "å™¨æ¢°",
    "åˆ¶è¯", "åˆ›æ–°è¯", "åŒ»é™¢", "å¥åº·", "ä¸´åºŠ", "FDA"
]


def get_headers():
    """è·å–éšæœºè¯·æ±‚å¤´"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }


def load_companies():
    """åŠ è½½ç›‘æ§ä¼ä¸šåˆ—è¡¨"""
    with open(CONFIG_DIR / "companies.json", "r", encoding="utf-8") as f:
        return json.load(f)["companies"]


# ==================== æ•°æ®æº 1: Firecrawl API ====================

def search_firecrawl(company_name: str) -> list:
    """Firecrawl API æœç´¢"""
    try:
        result = subprocess.run(
            ["pass", "show", "api/firecrawl"],
            capture_output=True, text=True
        )
        api_key = result.stdout.strip()
        
        if not api_key:
            return []
        
        query = f"{company_name} èèµ„"
        
        response = requests.post(
            "https://api.firecrawl.dev/v1/search",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={"query": query, "limit": 10, "lang": "zh"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            results = []
            for item in data.get("data", []):
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "snippet": item.get("description", ""),
                    "source": "Firecrawl",
                    "source_type": "search_engine"
                })
            return results
    except Exception as e:
        print(f"âš ï¸ Firecrawl é”™è¯¯: {e}", file=sys.stderr)
    return []


# ==================== æ•°æ®æº 2: 36æ°ª ====================

def search_36kr(company_name: str) -> list:
    """36æ°ªæœç´¢"""
    results = []
    try:
        url = f"https://36kr.com/search/articles/{quote(company_name)}"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            # 36æ°ªä½¿ç”¨ JavaScript æ¸²æŸ“ï¼Œå°è¯•ä» script æ ‡ç­¾æå–æ•°æ®
            soup = BeautifulSoup(response.text, "html.parser")
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« åˆ—è¡¨
            scripts = soup.find_all("script")
            for script in scripts:
                if script.string and "articleList" in script.string:
                    # æå– JSON æ•°æ®
                    match = re.search(r'"articleList":\s*(\[.*?\])', script.string)
                    if match:
                        try:
                            articles = json.loads(match.group(1))
                            for article in articles[:5]:
                                results.append({
                                    "title": article.get("title", ""),
                                    "url": f"https://36kr.com/p/{article.get('id', '')}",
                                    "snippet": article.get("summary", ""),
                                    "source": "36æ°ª",
                                    "source_type": "tech_media",
                                    "publish_time": article.get("publishTime", "")
                                })
                        except:
                            pass
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è§£æ HTML
            if not results:
                articles = soup.select("article, .article-item, .search-item")[:5]
                for article in articles:
                    title_elem = article.select_one("h3, .title, a")
                    if title_elem:
                        results.append({
                            "title": title_elem.get_text(strip=True),
                            "url": title_elem.get("href", ""),
                            "source": "36æ°ª",
                            "source_type": "tech_media"
                        })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ 36æ°ª é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== æ•°æ®æº 3: åŠ¨è„‰ç½‘ ====================

def search_vcbeat(company_name: str) -> list:
    """åŠ¨è„‰ç½‘æœç´¢ (åŒ»ç–—å‚ç›´åª’ä½“)"""
    results = []
    try:
        url = f"https://vcbeat.top/search?q={quote(company_name + ' èèµ„')}"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            articles = soup.select(".search-result-item, .article-item, article")[:5]
            
            for article in articles:
                title_elem = article.select_one("h2, h3, .title, a")
                snippet_elem = article.select_one("p, .summary, .desc")
                
                if title_elem:
                    results.append({
                        "title": title_elem.get_text(strip=True),
                        "url": title_elem.get("href", ""),
                        "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                        "source": "åŠ¨è„‰ç½‘",
                        "source_type": "healthcare_media"
                    })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ åŠ¨è„‰ç½‘ é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== æ•°æ®æº 4: ITæ¡”å­ ====================

def search_itjuzi(company_name: str) -> list:
    """ITæ¡”å­æœç´¢ (æŠ•èèµ„æ•°æ®åº“)"""
    results = []
    try:
        url = f"https://www.itjuzi.com/search?kw={quote(company_name)}"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æŸ¥æ‰¾èèµ„äº‹ä»¶
            events = soup.select(".event-item, .invest-item, .company-item")[:5]
            
            for event in events:
                title_elem = event.select_one("h3, .name, .title, a")
                info_elem = event.select_one(".info, .desc, .round")
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    info = info_elem.get_text(strip=True) if info_elem else ""
                    
                    results.append({
                        "title": f"{title} {info}",
                        "url": title_elem.get("href", ""),
                        "snippet": info,
                        "source": "ITæ¡”å­",
                        "source_type": "funding_database"
                    })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ ITæ¡”å­ é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== æ•°æ®æº 5: çˆ±ä¼æŸ¥ (ç™¾åº¦) ====================

def search_aiqicha(company_name: str) -> list:
    """çˆ±ä¼æŸ¥æœç´¢ (å·¥å•†ä¿¡æ¯)"""
    results = []
    try:
        url = f"https://aiqicha.baidu.com/s?q={quote(company_name)}"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æŸ¥æ‰¾å…¬å¸ä¿¡æ¯
            companies = soup.select(".search-item, .company-item")[:3]
            
            for company in companies:
                name_elem = company.select_one(".name, .title, h3")
                info_elem = company.select_one(".info, .detail")
                
                if name_elem:
                    # æå–å·¥å•†å˜æ›´ä¿¡æ¯
                    change_info = company.select_one(".change, .update")
                    
                    results.append({
                        "title": name_elem.get_text(strip=True),
                        "url": name_elem.get("href", ""),
                        "snippet": change_info.get_text(strip=True) if change_info else "",
                        "source": "çˆ±ä¼æŸ¥",
                        "source_type": "business_info"
                    })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ çˆ±ä¼æŸ¥ é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== æ•°æ®æº 6: æ–°æµªè´¢ç» ====================

def search_sina_finance(company_name: str) -> list:
    """æ–°æµªè´¢ç»æœç´¢"""
    results = []
    try:
        url = f"https://search.sina.com.cn/?q={quote(company_name + ' èèµ„')}&c=news&from=channel&ie=utf-8"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            articles = soup.select(".result, .box-result")[:5]
            
            for article in articles:
                title_elem = article.select_one("h2 a, .title a, a")
                snippet_elem = article.select_one("p, .content")
                
                if title_elem:
                    results.append({
                        "title": title_elem.get_text(strip=True),
                        "url": title_elem.get("href", ""),
                        "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                        "source": "æ–°æµªè´¢ç»",
                        "source_type": "finance_media"
                    })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ æ–°æµªè´¢ç» é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== æ•°æ®æº 7: æŠ•èµ„ç•Œ ====================

def search_pedaily(company_name: str) -> list:
    """æŠ•èµ„ç•Œæœç´¢ (PE/VC åª’ä½“)"""
    results = []
    try:
        url = f"https://www.pedaily.cn/search/?q={quote(company_name)}"
        response = requests.get(url, headers=get_headers(), timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            articles = soup.select(".news-item, .article-item, .search-item")[:5]
            
            for article in articles:
                title_elem = article.select_one("h3, .title, a")
                snippet_elem = article.select_one("p, .desc, .summary")
                
                if title_elem:
                    results.append({
                        "title": title_elem.get_text(strip=True),
                        "url": title_elem.get("href", ""),
                        "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                        "source": "æŠ•èµ„ç•Œ",
                        "source_type": "vc_media"
                    })
        
        time.sleep(random.uniform(1, 3))
    except Exception as e:
        print(f"âš ï¸ æŠ•èµ„ç•Œ é”™è¯¯: {e}", file=sys.stderr)
    return results


# ==================== èšåˆæœç´¢ ====================

def search_all_sources(company_name: str) -> list:
    """
    å¹¶è¡Œæœç´¢æ‰€æœ‰æ•°æ®æº
    """
    all_results = []
    
    # æ•°æ®æºåˆ—è¡¨
    sources = [
        ("Firecrawl", search_firecrawl),
        ("36æ°ª", search_36kr),
        ("åŠ¨è„‰ç½‘", search_vcbeat),
        ("ITæ¡”å­", search_itjuzi),
        ("æ–°æµªè´¢ç»", search_sina_finance),
        ("æŠ•èµ„ç•Œ", search_pedaily),
        # ("çˆ±ä¼æŸ¥", search_aiqicha),  # å·¥å•†ä¿¡æ¯ï¼Œå¯é€‰
    ]
    
    # å¹¶è¡Œæ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(func, company_name): name 
            for name, func in sources
        }
        
        for future in as_completed(futures, timeout=60):
            source_name = futures[future]
            try:
                results = future.result()
                if results:
                    print(f"  âœ“ {source_name}: {len(results)} æ¡", file=sys.stderr)
                    all_results.extend(results)
            except Exception as e:
                print(f"  âœ— {source_name}: {e}", file=sys.stderr)
    
    return all_results


# ==================== ä¿¡å·åˆ†æ ====================

def is_noise(text: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºå™ªéŸ³"""
    return any(noise in text for noise in NOISE_KEYWORDS)


def extract_round(text: str) -> str:
    """æå–èèµ„è½®æ¬¡"""
    patterns = [
        r"(ç§å­è½®|å¤©ä½¿è½®)",
        r"(Pre-[A-Z]\+?è½®?)",
        r"([A-Z]\+?è½®)",
        r"(æˆ˜ç•¥æŠ•èµ„|æˆ˜ç•¥èèµ„)",
        r"(IPO|ä¸Šå¸‚)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    return None


def extract_amount(text: str) -> str:
    """æå–èèµ„é‡‘é¢"""
    patterns = [
        r"(\d+(?:\.\d+)?)\s*(äº¿ç¾å…ƒ)",
        r"(\d+(?:\.\d+)?)\s*(äº¿äººæ°‘å¸|äº¿å…ƒ|äº¿)",
        r"(\d+(?:\.\d+)?)\s*(åƒä¸‡ç¾å…ƒ)",
        r"(\d+(?:\.\d+)?)\s*(åƒä¸‡äººæ°‘å¸|åƒä¸‡å…ƒ|åƒä¸‡)",
        r"(\d+(?:\.\d+)?)\s*(ç™¾ä¸‡ç¾å…ƒ|ä¸‡ç¾å…ƒ)",
        r"(\d+(?:\.\d+)?)\s*(ä¸‡äººæ°‘å¸|ä¸‡å…ƒ|ä¸‡)",
        r"\$(\d+(?:\.\d+)?)\s*(B|billion)",
        r"\$(\d+(?:\.\d+)?)\s*(M|million)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return f"{match.group(1)}{match.group(2)}"
    return None


def extract_investors(text: str) -> list:
    """æå–æŠ•èµ„æ–¹"""
    investors = []
    for keyword in INVESTOR_KEYWORDS:
        if keyword in text:
            # å°è¯•æå–å®Œæ•´æŠ•èµ„æ–¹åç§°
            pattern = rf"([\u4e00-\u9fa5]{{2,10}}{keyword})"
            matches = re.findall(pattern, text)
            investors.extend(matches)
    return list(set(investors))[:5]  # å»é‡ï¼Œæœ€å¤š5ä¸ª


def analyze_funding_signal(company_name: str, news_items: list) -> dict:
    """
    åˆ†æèèµ„ä¿¡å· (å¢å¼ºç‰ˆ)
    """
    signals = []
    total_confidence = 0
    sources_found = set()
    
    for item in news_items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        text = f"{title} {snippet}"
        source = item.get("source", "unknown")
        source_type = item.get("source_type", "unknown")
        
        # æ£€æŸ¥å™ªéŸ³
        if is_noise(text):
            continue
        
        # æ£€æŸ¥èèµ„å…³é”®è¯
        funding_matches = [kw for kw in FUNDING_KEYWORDS if kw in text]
        investor_matches = [kw for kw in INVESTOR_KEYWORDS if kw in text]
        
        if not funding_matches:
            continue
        
        # è®¡ç®—å•æ¡æ–°é—»ç½®ä¿¡åº¦
        item_confidence = 0
        item_confidence += len(funding_matches) * 10
        item_confidence += len(investor_matches) * 8
        
        # æå–è¯¦ç»†ä¿¡æ¯
        funding_round = extract_round(text)
        amount = extract_amount(text)
        investors = extract_investors(text)
        
        if funding_round:
            item_confidence += 20
        if amount:
            item_confidence += 15
        if investors:
            item_confidence += len(investors) * 5
        
        # æ•°æ®æºæƒé‡
        source_weights = {
            "funding_database": 1.5,  # ITæ¡”å­ç­‰èèµ„æ•°æ®åº“
            "healthcare_media": 1.3,  # åŠ¨è„‰ç½‘ç­‰åŒ»ç–—åª’ä½“
            "vc_media": 1.3,          # æŠ•èµ„ç•Œç­‰VCåª’ä½“
            "tech_media": 1.2,        # 36æ°ªç­‰ç§‘æŠ€åª’ä½“
            "search_engine": 1.0,     # Firecrawlæœç´¢
            "finance_media": 1.0,     # æ–°æµªè´¢ç»
            "business_info": 0.8,     # å·¥å•†ä¿¡æ¯
        }
        item_confidence *= source_weights.get(source_type, 1.0)
        
        signal = {
            "source": source,
            "source_type": source_type,
            "title": title[:100],
            "url": item.get("url", ""),
            "keywords": funding_matches[:5],
            "investors": investors,
            "confidence": min(int(item_confidence), 95)
        }
        
        if funding_round:
            signal["round"] = funding_round
        if amount:
            signal["amount"] = amount
        
        signals.append(signal)
        sources_found.add(source)
        total_confidence += item_confidence
    
    # å¤šæºéªŒè¯åŠ åˆ†
    if len(sources_found) >= 3:
        total_confidence += 25
    elif len(sources_found) >= 2:
        total_confidence += 15
    
    # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
    if signals:
        avg_confidence = total_confidence / len(signals)
        final_confidence = min(int(avg_confidence), 95)
    else:
        final_confidence = 0
    
    # å»é‡ä¿¡å· (æŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦)
    unique_signals = deduplicate_signals(signals)
    
    return {
        "company": company_name,
        "has_signal": len(unique_signals) > 0 and final_confidence >= 30,
        "confidence": final_confidence,
        "sources_count": len(sources_found),
        "sources": list(sources_found),
        "signals": unique_signals[:10],  # æœ€å¤š10æ¡
        "checked_at": datetime.now().isoformat()
    }


def deduplicate_signals(signals: list) -> list:
    """å»é‡ä¿¡å·"""
    seen_titles = set()
    unique = []
    
    for signal in sorted(signals, key=lambda x: x.get("confidence", 0), reverse=True):
        title = signal.get("title", "")[:50]  # å–å‰50å­—ç¬¦æ¯”è¾ƒ
        if title not in seen_titles:
            seen_titles.add(title)
            unique.append(signal)
    
    return unique


# ==================== ä¸»æµç¨‹ ====================

def check_company(company: dict) -> dict:
    """æ£€æŸ¥å•ä¸ªä¼ä¸š"""
    name = company["name"]
    
    print(f"ğŸ” æ£€æŸ¥: {name}", file=sys.stderr)
    
    # æœç´¢æ‰€æœ‰æ•°æ®æº
    news_items = search_all_sources(name)
    
    # åˆ†æä¿¡å·
    analysis = analyze_funding_signal(name, news_items)
    analysis["category"] = company.get("category", "æœªåˆ†ç±»")
    analysis["priority"] = company.get("priority", "normal")
    
    return analysis


def run_daily_check():
    """æ‰§è¡Œæ¯æ—¥æ£€æŸ¥"""
    companies = load_companies()
    today = datetime.now().strftime("%Y-%m-%d")
    
    results = {
        "date": today,
        "total": len(companies),
        "checked": 0,
        "signals_found": 0,
        "high_confidence": 0,
        "companies": []
    }
    
    for company in companies:
        try:
            analysis = check_company(company)
            results["companies"].append(analysis)
            results["checked"] += 1
            
            if analysis["has_signal"]:
                results["signals_found"] += 1
                if analysis["confidence"] >= 60:
                    results["high_confidence"] += 1
                    
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ {company['name']} å¤±è´¥: {e}", file=sys.stderr)
    
    # ä¿å­˜ç»“æœ
    results_dir = DATA_DIR / "funding_checks"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    results_file = results_dir / f"check_{today}.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return results


def format_telegram_report(results: dict) -> str:
    """æ ¼å¼åŒ– Telegram æŠ¥å‘Š"""
    report = f"""ğŸ¥ **åŒ»ç–—ä¼ä¸šèèµ„ç›‘æ§æ—¥æŠ¥**

ğŸ“… æ—¥æœŸ: {results['date']}
ğŸ“Š ç›‘æ§ä¼ä¸š: {results['total']} å®¶
âœ… å·²æ£€æŸ¥: {results['checked']} å®¶
ğŸš¨ å‘ç°ä¿¡å·: {results['signals_found']} ä¸ª
â­ é«˜ç½®ä¿¡åº¦: {results['high_confidence']} ä¸ª

"""
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    signals = [c for c in results['companies'] if c['has_signal']]
    signals.sort(key=lambda x: x['confidence'], reverse=True)
    
    if signals:
        report += "**ğŸ”” èèµ„ä¿¡å·**\n\n"
        
        for company in signals[:8]:  # æœ€å¤šæ˜¾ç¤º8ä¸ª
            confidence_icon = "ğŸ”´" if company['confidence'] >= 70 else "ğŸŸ¡" if company['confidence'] >= 50 else "ğŸŸ¢"
            report += f"{confidence_icon} **{company['company']}** ({company['category']})\n"
            report += f"ç½®ä¿¡åº¦: {company['confidence']}% | æ•°æ®æº: {company['sources_count']}ä¸ª\n"
            
            # æ˜¾ç¤ºæœ€é‡è¦çš„ä¿¡å·
            for signal in company['signals'][:2]:
                title = signal['title'][:45] + "..." if len(signal['title']) > 45 else signal['title']
                report += f"â€¢ {title}\n"
                
                details = []
                if signal.get('round'):
                    details.append(f"è½®æ¬¡: {signal['round']}")
                if signal.get('amount'):
                    details.append(f"é‡‘é¢: {signal['amount']}")
                if signal.get('investors'):
                    details.append(f"æŠ•èµ„æ–¹: {', '.join(signal['investors'][:2])}")
                
                if details:
                    report += f"  {' | '.join(details)}\n"
            
            report += "\n"
    else:
        report += "**ğŸ“­ æš‚æ— èèµ„ä¿¡å·**\n\n"
    
    # æ•°æ®æºç»Ÿè®¡
    all_sources = set()
    for c in results['companies']:
        all_sources.update(c.get('sources', []))
    
    report += f"**ğŸ“¡ æ•°æ®æº**: {', '.join(all_sources) if all_sources else 'æ— '}\n"
    report += f"---\n_ç›‘æ§æ—¶é—´: {datetime.now().strftime('%H:%M')}_"
    
    return report


def push_to_telegram(message: str):
    """æ¨é€åˆ° Telegram"""
    push_script = Path.home() / "clawd" / "skills" / "telegram-push" / "telegram-push.sh"
    
    if push_script.exists():
        subprocess.run([str(push_script), message], check=True)
        print("âœ… å·²æ¨é€åˆ° Telegram", file=sys.stderr)
    else:
        print("âš ï¸ telegram-push.sh ä¸å­˜åœ¨", file=sys.stderr)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ»ç–—ä¼ä¸šèèµ„æ£€æµ‹ v2.0")
    parser.add_argument("--check", action="store_true", help="æ‰§è¡Œæ£€æŸ¥")
    parser.add_argument("--push", action="store_true", help="æ¨é€æŠ¥å‘Š")
    parser.add_argument("--company", type=str, help="æ£€æŸ¥å•ä¸ªä¼ä¸š")
    
    args = parser.parse_args()
    
    if args.company:
        company = {"name": args.company, "full_name": args.company}
        result = check_company(company)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    elif args.check:
        results = run_daily_check()
        report = format_telegram_report(results)
        print(report)
        
        if args.push:
            push_to_telegram(report)
    else:
        parser.print_help()
